################# Práctico PLN 1ª Parte #######################################
library(skimr)
library(tidyverse)
library(janitor)
library(hrbrthemes)
library(lubridate)
library(summarytools)
library(dplyr)
library(ggsci)
library(ggplot2)

#Como en todo ejercicio de análisis de texto a través de lenguaje natural, vamos a explorar el df con el 
#objetivo puesto en entender de qué trata este corpus de texto. Por eso, en principio, no damos nunguna pista de 
#qué contenidos tiene este corpus y partimos de la base de que hay que descubrirlo.

#Cargamos el df

library(readr)

practicopln <- read_csv("R/Curso instrumenta/Curso instrumenta/Clase-10 Practica guiada/practicopln.csv")
View(practicopln)

#1)Superficialmente, ¿De què trata este corpus? Utilizar funciones exploratorias

summary(practicopln)

head(practicopln)

glimpse(practicopln)

#2) ¿En qué otros idiomas fueron escritos estos tweets? Me quedo con aquellos tweets que solo han sido escritos
#en español

str(practicopln)

freq(practicopln$lang)

unique(practicopln$lang)

df <- practicopln %>%
  filter(lang == "es")

df


#3)¿Cuáles son las 50 palabras más frecuentes para estos tweets?

my_stopwords <- read_csv("R/Curso instrumenta/Curso instrumenta/Clase-03.Introduccion a Tidyverse. Introduccion al analisis de texto/my_stopwords.csv")

desecho <-  data.frame(word= c("t.co", "https"))

df_1 <- df %>% select(text) #esto lo hago para quedarme solo con los tweets

view(df_1)

library(tidytext)

palabras <- df_1 %>% 
  unnest_tokens(word, text) %>% 
  anti_join(my_stopwords) %>%
  count(word, sort = TRUE)

view(palabras)

top_50 <- palabras %>% top_n(50)

top_50

#4)  Ahora, podemos hacernos una idea de lo que tratan estos tweets, pero podríamos ir un poco más allá con 
# otras herramientas. También podríamos explorar n-grams. Aquí podríamos explorar el número de n-grams que 
#quisiéramos, pero lo recomendable en PLN es explorar hasta 3-grams y no más ya que suele ser infructuoso. 
#Veamos entonces ¿cuáles son los 10 bigramas y los 10 trigramas más comunes? Recordemos que aquí la estrategia
#para limpiar palabras vacías no es anti_join

bigramas <- df_1  %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)


bigramas <- bigramas %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigramas <- bigramas %>%
  filter(!word1 %in% my_stopwords$word) %>%
  filter(!word2 %in% my_stopwords$word)


bigramas <- bigramas %>%
  unite(bigram, word1, word2, sep = " ")
bigramas %>% top_n(10)

#trigramas

trigramas <- df_1  %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  count(trigram, sort = TRUE) 

trigramas <- trigramas %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigramas <- trigramas %>%
  filter(!word1 %in% my_stopwords$word,
         !word2 %in% my_stopwords$word,
         !word3 %in% my_stopwords$word)

trigramas <- trigramas %>%
  unite(trigram, word1, word2, word3, sep = " ")
trigramas %>% top_n(10)


#5) ¿Qué nodos de significado existen y cuáles son son los núcleos más importantes? Lo graficamos a través de 
#redes de n.gramas. Recordar que para graficar estas redes, los biogrmas tienen que estar separados.

install.packages("igraph")
library(igraph)

nodos_bigramas <- bigramas %>% 
  filter(n > 1000) %>%
  graph_from_data_frame()
print(nodos_bigramas)

nodos_trigramas <- trigramas%>% 
  filter(n > 1000) %>%
  graph_from_data_frame()
print(nodos_trigramas)

#GR?FICOS

library(ggplot2)
library(ggraph)

 

plot1 <- ggraph(nodos_bigramas, layout= "fr") +
  geom_edge_link(color= "#870C37") +
  geom_node_point(color="#F908BF") +
  geom_node_text(aes(label = name), color= "#4E0447", vjust = 1, hjust = 1) 


plot2 <- ggraph(nodos_trigramas, layout= "fr") +
  geom_edge_link(color= "#870C37") +
  geom_node_point(color="#F908BF") +
  geom_node_text(aes(label = name), color= "#4E0447", vjust = 1, hjust = 1)

plot2
